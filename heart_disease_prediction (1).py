# -*- coding: utf-8 -*-
"""Heart Disease Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uhgrbbwYj-am-YJLMWy-JH5msYtnRBBY
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

df=pd.read_csv("/content/framingham.csv")
df

df.head()

df.tail()

df.describe().T

df.info()
df.shape

df.isnull().sum()

columns_to_fill = ['glucose', 'education', 'BPMeds', 'cigsPerDay', 'totChol', 'BMI', 'heartRate']
df[columns_to_fill] = df[columns_to_fill].fillna(df[columns_to_fill].mean())

df[columns_to_fill]

missing_values=df.isnull().sum()

missing_values

df.mean(numeric_only=True)

df.median(numeric_only=True)

df.mode(numeric_only=True)

df.std(numeric_only=True)

df.var(numeric_only=True)

df.skew(numeric_only=True)

df.kurt(numeric_only=True)

categorical_columns=['male','currentSmoker','BPMeds','prevalentStroke','prevalentHyp','diabetes','TenYearCHD']
categorical_columns

continuous_columns=['age','education','cigsPerDay','totChol','sysBP','diaBP','BMI','heartRate','glucose']
continuous_columns

#countplot for categorical columns

for col in categorical_columns:
  plt.figure(figsize=(7,5))
  sns.countplot(x=col,data=df)
  plt.title(f'countplot of {col}')
  plt.show()

#histplot for numeric columns

for col in continuous_columns:
  plt.figure(figsize=(7,5))
  sns.histplot(x=col,data=df)
  plt.title(f"histplot of {col}")
  plt.show()

#lineplot for continuous columns

for col in continuous_columns:
  if col!='age':
     plt.figure(figsize=(7,5))
  # To plot a single continuous variable as a line plot, we set it as the y-axis.
  # The x-axis will default to the DataFrame's index.
     sns.lineplot(x='age',y=col,hue='TenYearCHD',data=df)
     plt.title(f"lineplot of {col}")
     plt.show()

#barplot for categorical columns

for cat_col in categorical_columns:
    for cont_col in continuous_columns:
        plt.figure(figsize=(9,7))
        sns.barplot(data=df, x=cat_col, y=cont_col)
        plt.title(f'Barplot of {cont_col} by {cat_col}')
        plt.show()

#piechart for categorical columns

for col in categorical_columns:
  plt.figure(figsize=(7,5))
  # Calculate value counts for the column to get sizes for the pie wedges
  sizes = df[col].value_counts()
  # Use the index of the value counts as labels for the pie wedges
  labels = sizes.index
  plt.pie(x=sizes, labels=labels, autopct='%1.1f%%')
  plt.title(f"piechart of {col}")
  plt.show()

#histogram for continuous columns

for col in continuous_columns:
  plt.figure(figsize=(9,8))
  plt.hist(df[col])
  plt.title(f"hist0gram of {col}")
  plt.show()

#boxplot for continuous columns

for col in continuous_columns:
    plt.figure(figsize=(7,5))
    sns.boxplot(x=df[col])
    plt.title(f"BoxPlot of {col}")
    plt.show()

Q3 = df[col].quantile(0.75)
Q1 = df[col].quantile(0.25)
IQR = Q3 - Q1

lower_bound=Q3-1.5*IQR
upper_bound=Q1+1.5*IQR

outliers=df[(df[col]<lower_bound) | (df[col]>upper_bound)]

print(f"no of outliers: {outliers}")
print(outliers)

df.plot(x='age',y='TenYearCHD',kind='scatter')
plt.show()

df.plot(x='diabetes',y='TenYearCHD',kind='scatter')
plt.show()

plt.figure(figsize=(10,9))
sns.pairplot(df[continuous_columns])
plt.show()

corr=df.corr()

plt.figure(figsize=(10,9))
sns.heatmap(corr,annot=True,cmap='Blues')
plt.show()

The correlation heatmap visualizes the relationship between numerical features.

Features with higher positive or negative correlation values may significantly influence the target variable. Highly correlated independent features may indicate multicollinearity and can be considered for removal or regularization during model building

X=df.drop('TenYearCHD', axis=1)

X

Y=df['TenYearCHD']

Y

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=50)

x_train.shape,x_test.shape,y_train.shape,y_test.shape

from sklearn.preprocessing import StandardScaler

scaler=StandardScaler()

x_train_scaler=scaler.fit_transform(x_train)
x_test_scaler=scaler.transform(x_test)

from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier

from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score,confusion_matrix,classification_report,roc_auc_score

lr=LogisticRegression(max_iter=500)

model=lr.fit(x_train_scaler,y_train)

y_pred=lr.predict(x_test_scaler)

acc=accuracy_score(y_pred,y_test)
pre=precision_score(y_pred,y_test,average='weighted')
rec=recall_score(y_pred,y_test,average='weighted')
f1=f1_score(y_pred,y_test,average='weighted')
roc_auc=roc_auc_score(y_pred,y_test)

print("Accuracy:", acc)
print("Precision:", pre)
print("Recall:", rec)
print("f1_score:", f1)
print("Roc-Auc:",roc_auc)

print("confusion matrix\n", confusion_matrix(y_pred,y_test))
print("classification report\n", classification_report(y_pred,y_test))

svm=SVC(kernel='rbf', C=2)

model1=svm.fit(x_train_scaler,y_train)

y_pred1=svm.predict(x_test_scaler)

acc=accuracy_score(y_pred1,y_test)
pre=precision_score(y_pred1,y_test,average='weighted')
rec=recall_score(y_pred1,y_test,average='weighted')
f1=f1_score(y_pred1,y_test,average='weighted')
roc_auc=roc_auc_score(y_pred1,y_test)

print("Accuracy:", acc)
print("Precision:", pre)
print("Recall:", rec)
print("f1_score:", f1)
print("Roc-Auc:",roc_auc)

print("confusion matrix\n", confusion_matrix(y_pred,y_test))
print("classification report\n", classification_report(y_pred,y_test))

dt=DecisionTreeClassifier(criterion ='entropy',random_state=0,max_depth=6)

model2=dt.fit(x_train_scaler,y_train)

y_pred2=dt.predict(x_test_scaler)

acc=accuracy_score(y_pred2,y_test)
pre=precision_score(y_pred2,y_test,average='weighted')
rec=recall_score(y_pred2,y_test,average='weighted')
f1=f1_score(y_pred2,y_test,average='weighted')
roc_auc=roc_auc_score(y_pred2,y_test)

print("Accuracy:", acc)
print("Precision:", pre)
print("Recall:", rec)
print("f1_score:", f1)
print("Roc-Auc:",roc_auc)

print("confusion matrix\n", confusion_matrix(y_pred,y_test))
print("classification report\n", classification_report(y_pred,y_test))

rf=RandomForestClassifier(n_estimators=20, random_state=12,max_depth=5)

model3=rf.fit(x_train_scaler,y_train)

y_pred3=rf.predict(x_test_scaler)

acc=accuracy_score(y_pred3,y_test)
pre=precision_score(y_pred3,y_test,average='weighted')
rec=recall_score(y_pred3,y_test,average='weighted')
f1=f1_score(y_pred3,y_test,average='weighted')
roc_auc=roc_auc_score(y_pred3,y_test)

print("Accuracy:", acc)
print("Precision:", pre)
print("Recall:", rec)
print("f1_score:", f1)
print("Roc-Auc:",roc_auc)

print("confusion matrix\n", confusion_matrix(y_pred,y_test))
print("classification report\n", classification_report(y_pred,y_test))

lr_acc_score=84.4
svc_acc_score=84.0
dt_acc_score=83.8
rf_acc_score=84.1

model_ev = pd.DataFrame({'Model': ['Logistic Regression','Support Vector Machine', 'Decision Tree', 'Random Forest'],
                       'Accuracy': [lr_acc_score, svc_acc_score, dt_acc_score, rf_acc_score]})
model_ev

plt.figure(figsize=(15,5))
plt.title("barplot Represent Accuracy of different models")
plt.xlabel("Accuracy %")
plt.ylabel("Algorithms")
plt.bar(model_ev['Model'],model_ev['Accuracy'])
plt.show()

By comparing all the models LogisticRegression will get high accuracy score(84.43%) and f1_score(90.0s%).